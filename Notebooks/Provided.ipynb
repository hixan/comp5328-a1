{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 1: Non-negative Matrix Factorization\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Semester 2, 2020)**\n",
    "\n",
    "In this ipython notebook, we provide some example code for assignment1.\n",
    "+ Load Data.\n",
    "    - ORL dataset. \n",
    "    - Extended YaleB dataset. \n",
    "    - AR dataset (**optional**).\n",
    "+ Perform Evaluation. \n",
    "   - Relative Reconstruction Errors.\n",
    "   - Accuracy, NMI (**optional**).\n",
    "\n",
    "Lecturer: Tongliang Liu.\n",
    "\n",
    "Tutors: Nicholas James, Songhua Wu, Xuefeng Li, Yu Yao.\n",
    "\n",
    "**Note: All datasets can be used only for this assignment and you are not allowed to distribute these datasets. If you want to use AR dataset, you need to apply it by yourself (we do not provide AR dataset due to the problem of license, please find more details in http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxrwxrwx@ 41 ybs  staff  1312 Aug 25 16:59 \u001b[30m\u001b[43mCroppedYaleB\u001b[m\u001b[m\r\n",
      "drwx------@ 44 ybs  staff  1408 Aug 24 15:37 \u001b[34mORL\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tree structure of data folder.\n",
    "├── CroppedAR\n",
    "    ├── M-001-01.bmp\n",
    "    ├── M-001-01.txt\n",
    "    ├── M-001-02.bmp\n",
    "    ├── M-001-02.txt\n",
    "    ├── ...\n",
    "├── CroppedYaleB\n",
    "│   ├── yaleB01\n",
    "│   ├── yaleB02\n",
    "│   ...\n",
    "│   ├── yaleB38\n",
    "│   └── yaleB39\n",
    "└── ORL\n",
    "    ├── s1\n",
    "    ├── s2\n",
    "    ├── s3\n",
    "    ├── ...\n",
    "    ├── s40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load ORL Dataset and Extended YaleB Dataset.\n",
    "+ ORL dataset contains ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel. To further reduce the computation complexity, you can resize all images to 30x37 pixels.\n",
    "\n",
    "+ Extended YaleB dataset contains 2414 images of 38 human subjects under 9 poses and 64 illumination conditions. All images are manually aligned, cropped, and then resized to 168x192 pixels. To further reduce the computation complexity, you can resize all images to 42x48 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_data(root='data/CroppedYaleB', reduce=4):\n",
    "    \"\"\" \n",
    "    Load ORL (or Extended YaleB) dataset to numpy array.\n",
    "    \n",
    "    Args:\n",
    "        root: path to dataset.\n",
    "        reduce: scale factor for zooming out images.\n",
    "        \n",
    "    \"\"\" \n",
    "    images, labels = [], []\n",
    "\n",
    "    for i, person in enumerate(sorted(os.listdir(root))):\n",
    "        \n",
    "        if not os.path.isdir(os.path.join(root, person)):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(os.path.join(root, person)):    \n",
    "            \n",
    "            # Remove background images in Extended YaleB dataset.\n",
    "            if fname.endswith('Ambient.pgm'):\n",
    "                continue\n",
    "            \n",
    "            if not fname.endswith('.pgm'):\n",
    "                continue\n",
    "                \n",
    "            # load image.\n",
    "            img = Image.open(os.path.join(root, person, fname))\n",
    "            img = img.convert('L') # grey image.\n",
    "\n",
    "            # reduce computation complexity.\n",
    "            img = img.resize([s//reduce for s in img.size])\n",
    "\n",
    "            # TODO: preprocessing.\n",
    "\n",
    "            # convert image to numpy array.\n",
    "            img = np.asarray(img).reshape((-1,1))\n",
    "\n",
    "            # collect data and label.\n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "\n",
    "    # concate all images and labels.\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORL dataset: X.shape = (2576, 400), Y.shape = (400,)\n",
      "Extended YalB dataset: X.shape = (2016, 2414), Y.shape = (2414,)\n"
     ]
    }
   ],
   "source": [
    "# Load ORL dataset.\n",
    "X, Y = load_data(root='data/ORL', reduce=2)\n",
    "print('ORL dataset: X.shape = {}, Y.shape = {}'.format(X.shape, Y.shape))\n",
    "\n",
    "# Load Extended YaleB dataset.\n",
    "X, Y = load_data(root='data/CroppedYaleB', reduce=4)\n",
    "print('Extended YalB dataset: X.shape = {}, Y.shape = {}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load AR Dataset (Optional) \n",
    "AR dataset contains 2600 images of 100 individuals (50 male and 50 female). All images have been cropped and resized to 120x165 pixels. To further reduce the computation complexity, you can resize all images to 40x55 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_AR(root='data/CroppedAR', reduce=3):\n",
    "    \"\"\" \n",
    "    Load AR dataset to numpy array.\n",
    "    \n",
    "    Args:\n",
    "        root: path to AR dataset.\n",
    "        reduce: scale factor for zooming out images.\n",
    "        \n",
    "    \"\"\" \n",
    "    images, labels = [], []\n",
    "    \n",
    "    for fname in os.listdir(root):\n",
    "        \n",
    "        if not fname.endswith('.bmp'):\n",
    "            continue\n",
    "        \n",
    "        # get label.\n",
    "        label = int(fname[2:5])\n",
    "        if fname[0] == 'W': # start from 50\n",
    "            label += 50\n",
    "        \n",
    "        # load image.\n",
    "        img = Image.open(os.path.join(root, fname))\n",
    "        img = img.convert('L') # grey\n",
    "        \n",
    "        # reduce computation complexity.\n",
    "        img = img.resize([s//reduce for s in img.size])\n",
    "   \n",
    "        # TODO: preprocessing.\n",
    "        \n",
    "        # convert image to numpy array.\n",
    "        img = np.asarray(img).reshape((-1,1))\n",
    "        \n",
    "        # collect data and label.\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "        \n",
    "    # concate all images and labels.\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, Y = load_data_AR(root='data/CroppedAR', reduce=3)\n",
    "# print('AR dataset: X.shape = {}, Y.shape = {}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics\n",
    "\n",
    "\n",
    "### 2.1 Relative Reconstruction Errors (RRE)\n",
    "\n",
    "To compare the robustness of different NMF algorithms, you can use the ```relative reconstruction errors```. Let $V$ denote the contaminated dataset (by adding noise), and $\\hat{V}$\n",
    " denote the clean dataset. Let $W$ and $H$ denote the factorization results on $V$, the ``relative reconstruction errors`` then can be defined as follows:\n",
    " \\begin{equation}\n",
    "    RRE = \\frac{ \\| \\hat{V} - WH \\|_F }{ \\| \\hat{V} \\|_F}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Load ORL dataset ...\n",
      "V_hat.shape=(1110, 400), Y_hat.shape=(400,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Apply NMF ...\n",
      "W.shape=(1110, 40), H.shape=(40, 400)\n",
      "==> Evaluate RRE ...\n",
      "RRE = 0.22688415691467015\n"
     ]
    }
   ],
   "source": [
    "# Load dataset.\n",
    "print('==> Load ORL dataset ...')\n",
    "V_hat, Y_hat = load_data('data/ORL', reduce=3)\n",
    "print('V_hat.shape={}, Y_hat.shape={}'.format(V_hat.shape, Y_hat.shape))\n",
    "\n",
    "# Add Noise.\n",
    "V_noise = np.random.rand(*V_hat.shape) * 40\n",
    "V = V_hat + V_noise\n",
    "\n",
    "# Plot result.\n",
    "import matplotlib.pyplot as plt\n",
    "img_size = [i//3 for i in (92, 112)] # ORL\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_noise[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Noise')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(Noise)')\n",
    "plt.show()\n",
    "\n",
    "# TODO: you should implement NMF algorithms by yourself.\n",
    "print('==> Apply NMF ...')\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=len(set(Y_hat))) # set n_components to num_classes.\n",
    "W = model.fit_transform(V)\n",
    "H = model.components_\n",
    "print('W.shape={}, H.shape={}'.format(W.shape, H.shape))\n",
    "\n",
    "# Evaluate relative reconstruction errors.\n",
    "print('==> Evaluate RRE ...')\n",
    "RRE = np.linalg.norm(V_hat - W.dot(H)) / np.linalg.norm(V_hat)\n",
    "print('RRE = {}'.format(RRE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate Clustering Performance\n",
    "\n",
    "1. Accuracy.\n",
    "    \n",
    "    $$ Acc(Y, Y_{pred}) = \\frac{1}{n}\\sum\\limits_{i=1}^n 1\\{Y_{pred}(i) == Y(i)\\}$$\n",
    "        \n",
    "2. Normalized Mutual Information (NMI).\n",
    "\n",
    "    $$ NMI(Y, Y_{pred}) = \\frac{2 * I(Y, Y_{pred})}{H(Y) + H(Y_{pred})} $$\n",
    "    \n",
    "   where $ I(\\cdot,\\cdot) $ is mutual information and $ H(\\cdot) $ is entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluate Acc and NMI ...\n",
      "Acc(NMI) = 0.5875 (0.7431)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def assign_cluster_label(X, Y):\n",
    "    kmeans = KMeans(n_clusters=len(set(Y))).fit(X)\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    for i in set(kmeans.labels_):\n",
    "        ind = kmeans.labels_ == i\n",
    "        Y_pred[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "    return Y_pred\n",
    "\n",
    "print('==> Evaluate Acc and NMI ...')\n",
    "\n",
    "# Assign cluster labels.\n",
    "Y_pred = assign_cluster_label(H.T, Y_hat)\n",
    "\n",
    "acc = accuracy_score(Y_hat, Y_pred)\n",
    "nmi = normalized_mutual_info_score(Y_hat, Y_pred)\n",
    "print('Acc(NMI) = {:.4f} ({:.4f})'.format(acc, nmi))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
